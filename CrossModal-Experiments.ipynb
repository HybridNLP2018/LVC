{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from aux_functions import get_n_images, get_cross_model\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv8rfLVKgQ2"
   },
   "source": [
    "## Experiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"scigraph\"\n",
    "#corpus = \"semantic-scholar\"\n",
    "\n",
    "embeddingsType = \"scratch\"\n",
    "\n",
    "print(\"Selected the \"+corpus+\" corpus with embeddings from \" +embeddingsType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv8rfLVKgQ2"
   },
   "source": [
    "## Input files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dmRTQW95USVT"
   },
   "outputs": [],
   "source": [
    "h5path ='./databases/cross-corpus-'+corpus+'.h5'\n",
    "vocabulary_tokens = './vocabularies/cross-vocab-tokens-'+corpus+'.pkl'\n",
    "\n",
    "max_sequence_length = 1000\n",
    "dim = 300\n",
    "batchSize= 32\n",
    "n_images = get_n_images(corpus)\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKEK7CfXUSVq"
   },
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocabulary_tokens, 'rb') as fdict:\n",
    "        word_index_tokens = pickle.load(fdict)\n",
    "fdict.close()\n",
    "print(\"Found \" + str(len(word_index_tokens)) + \" unique tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4lSU1ebUSVN"
   },
   "source": [
    "## Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6hCoLNW9USVa"
   },
   "outputs": [],
   "source": [
    "def generator (h5path, indexes,batchSize, shuffle):\n",
    "  db = h5py.File(h5path, \"r\")\n",
    "  while True:\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indexes)\n",
    "    for i in range(0, len(indexes), batchSize):\n",
    "        batch_indexes = indexes[i:i+batchSize]\n",
    "        batch_indexes.sort()\n",
    "        \n",
    "        bx1 = db[\"captions_tokens\"][batch_indexes,:]\n",
    "        bx2 = db[\"images\"][batch_indexes,:,:,:]\n",
    "        by = db[\"labels\"][batch_indexes,:]\n",
    "            \n",
    "        yield ([bx1, bx2], by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKEK7CfXUSVq"
   },
   "source": [
    "## Training model (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1952
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 614765,
     "status": "error",
     "timestamp": 1526891363734,
     "user": {
      "displayName": "RaÃºl Ortega",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118338186511913721141"
     },
     "user_tz": -120
    },
    "id": "FbUsHU1pUSVq",
    "outputId": "2238c2a0-f3e2-485b-959e-6401eafcb811",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "fold = 1\n",
    "print (\"Number of images: \"+str(n_images))\n",
    "print (\"Number of classes: 2\\n\")\n",
    "for train, test in kfold.split([None] * n_images):\n",
    "    print(\"FOLD: \" +str(fold))\n",
    "    print(\"Training with \"+ str(len(train))+ \" samples and evaluating with \"+str(len(test)))\n",
    "    \n",
    "    model = get_cross_model (word_index_tokens, max_sequence_length, dim)\n",
    "    \n",
    "    adam = optimizers.Adam(lr=1e-4, decay=1e-5)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['categorical_accuracy'])\n",
    "  \n",
    "    model.fit_generator(generator(h5path,train,batchSize, shuffle=True), epochs=epochs, steps_per_epoch = len(train)//batchSize, validation_data=(generator(h5path,test,batchSize, shuffle=False)), validation_steps= len(test)//batchSize)\n",
    "      \n",
    "    db = h5py.File(h5path, \"r\")\n",
    "    labels_test = db[\"labels\"][test,:]\n",
    "    db.close()\n",
    "\n",
    "    pred = model.predict_generator(generator(h5path, test, batchSize=batchSize, shuffle=False),steps = len(test)//batchSize)\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    print(classification_report(labels_test[0:batchSize*(len(test)//batchSize)], pred, digits=4))\n",
    "    precisions.append(precision_score(labels_test[0:batchSize*(len(test)//batchSize)], pred, average=\"weighted\"))\n",
    "    recalls.append(recall_score(labels_test[0:batchSize*(len(test)//batchSize)], pred, average=\"weighted\"))\n",
    "    f1s.append(f1_score(labels_test[0:batchSize*(len(test)//batchSize)], pred, average=\"weighted\"))\n",
    "    fold = fold +1\n",
    "print(\"Precision: %.4f (+/- %.2f)\" % (np.mean(precisions), np.std(precisions)))\n",
    "print(\"Recall: %.4f (+/- %.2f)\" % (np.mean(recalls), np.std(recalls)))\n",
    "print(\"F1 Score: %.4f (+/- %.2f)\" % (np.mean(f1s), np.std(f1s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZfQTLqW9HAJ8",
    "YSxJx5b4bRyC",
    "4AQchbAKHVSk",
    "2fJlkTLGSXKX",
    "KftAruKttTBk",
    "MYRiYDmf9l_l",
    "D8jhia_wphXd",
    "KVdtrbJCpmJW",
    "vqxj9bWWpmJY",
    "FXI8meXQeqKA",
    "o_gDBeWxvEX2"
   ],
   "default_view": {},
   "name": "TFG.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
