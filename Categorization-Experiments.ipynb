{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from aux_functions import get_cat_dim, get_n_images, get_cat_captions_model, get_cat_figures_model\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv8rfLVKgQ2"
   },
   "source": [
    "## Experiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected the cross weights for categorizing captions\n"
     ]
    }
   ],
   "source": [
    "weights = \"cross\"\n",
    "#weights = \"cross-vecsi\"\n",
    "\n",
    "print(\"Selected the \"+weights+\" weights for categorizing captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv8rfLVKgQ2"
   },
   "source": [
    "## Input files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dmRTQW95USVT"
   },
   "outputs": [],
   "source": [
    "h5path=\"./databases/cat-corpus-captions.h5\"\n",
    "vocabulary_tokens = './vocabularies/cross-vocab-tokens-scigraph.pkl'\n",
    "vocabulary_syncons = './vocabularies/cross-vocab-syncons-scigraph.pkl'\n",
    "\n",
    "max_sequence_length = 1000\n",
    "dim = get_cat_dim(weights)\n",
    "batchSize= 128\n",
    "n_images = 82396\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKEK7CfXUSVq"
   },
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102167 unique tokens\n",
      "Found 46577 unique syncons\n"
     ]
    }
   ],
   "source": [
    "with open(vocabulary_tokens, 'rb') as fdict:\n",
    "        word_index_tokens = pickle.load(fdict)\n",
    "fdict.close()\n",
    "print(\"Found \" + str(len(word_index_tokens)) + \" unique tokens\")\n",
    "with open(vocabulary_syncons, 'rb') as fdict:\n",
    "        word_index_syncons = pickle.load(fdict)\n",
    "fdict.close()\n",
    "print(\"Found \" + str(len(word_index_syncons)) + \" unique syncons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4lSU1ebUSVN"
   },
   "source": [
    "## Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6hCoLNW9USVa"
   },
   "outputs": [],
   "source": [
    "def generator (h5path, indexes,batchSize, shuffle):\n",
    "  db = h5py.File(h5path, \"r\")\n",
    "  while True:\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indexes)\n",
    "    for i in range(0, len(indexes), batchSize):\n",
    "        batch_indexes = indexes[i:i+batchSize]\n",
    "        batch_indexes.sort()\n",
    "        \n",
    "        if (weights == \"cross\"):\n",
    "            bx = db[\"captions_tokens\"][batch_indexes,:]\n",
    "            by = db[\"labels\"][batch_indexes,:]\n",
    "            \n",
    "            yield (bx, by)\n",
    "            \n",
    "        if (weights == \"cross-vecsi\"):\n",
    "            bx1 = db[\"captions_tokens\"][batch_indexes,:]\n",
    "            bx2 = db[\"captions_tokens\"][batch_indexes,:]\n",
    "            bx3 = db[\"captions_syncons\"][batch_indexes,:]\n",
    "            by = db[\"labels\"][batch_indexes,:]\n",
    "\n",
    "            yield ([bx1, bx2, bx3], by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKEK7CfXUSVq"
   },
   "source": [
    "## Training model (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1952
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 614765,
     "status": "error",
     "timestamp": 1526891363734,
     "user": {
      "displayName": "Raúl Ortega",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118338186511913721141"
     },
     "user_tz": -120
    },
    "id": "FbUsHU1pUSVq",
    "outputId": "2238c2a0-f3e2-485b-959e-6401eafcb811",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "fold = 1\n",
    "print (\"Number of images: \"+str(n_images))\n",
    "print (\"Number of classes: 5\\n\")\n",
    "for train, test in kfold.split([None] * n_images):\n",
    "    print(\"FOLD: \" +str(fold))\n",
    "    print(\"Training with \"+ str(len(train))+ \" samples and evaluating with \"+str(len(test)))\n",
    "    \n",
    "    model = get_cat_captions_model (weights, word_index_tokens, word_index_syncons, max_sequence_length, dim)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop', metrics=['categorical_accuracy'])\n",
    "  \n",
    "    model.fit_generator(generator(h5path,train,batchSize, shuffle=True), epochs=epochs, steps_per_epoch = len(train)//batchSize, validation_data=(generator(h5path,test,batchSize, shuffle=False)), validation_steps= len(test)//batchSize)\n",
    "      \n",
    "    db = h5py.File(h5path, \"r\")\n",
    "    labels_test = db[\"labels\"][test,:]\n",
    "    db.close()\n",
    "\n",
    "    pred = model.predict_generator(generator(h5path, test, batchSize=batchSize, shuffle=False),steps = len(test)//batchSize)\n",
    "    maximos = np.argmax(pred,axis=1)\n",
    "    predNew = np.zeros(np.shape(pred))\n",
    "    for i in range(len(predNew)):\n",
    "        predNew[i,maximos[i]]=1\n",
    "    print(classification_report(labels_test[0:batchSize*(len(test)//batchSize)], predNew, digits=4))\n",
    "    precisions.append(precision_score(labels_test[0:batchSize*(len(test)//batchSize)], predNew, average=\"weighted\"))\n",
    "    recalls.append(recall_score(labels_test[0:batchSize*(len(test)//batchSize)], predNew, average=\"weighted\"))\n",
    "    f1s.append(f1_score(labels_test[0:batchSize*(len(test)//batchSize)], predNew, average=\"weighted\"))\n",
    "    fold = fold +1\n",
    "print(\"Precision: %.4f (+/- %.2f)\" % (np.mean(precisions), np.std(precisions)))\n",
    "print(\"Recall: %.4f (+/- %.2f)\" % (np.mean(recalls), np.std(recalls)))\n",
    "print(\"F1 Score: %.4f (+/- %.2f)\" % (np.mean(f1s), np.std(f1s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv8rfLVKgQ2"
   },
   "source": [
    "## Experiment selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected the cross weights for categorizing figures\n"
     ]
    }
   ],
   "source": [
    "weights = \"cross\"\n",
    "#weights = \"cross-vecsi\"\n",
    "\n",
    "print(\"Selected the \"+weights+\" weights for categorizing figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vvv8rfLVKgQ2"
   },
   "source": [
    "## Input files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dmRTQW95USVT"
   },
   "outputs": [],
   "source": [
    "h5path=\"./databases/cat-corpus-figures.h5\"\n",
    "\n",
    "batchSize= 32\n",
    "n_images = 82396\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4lSU1ebUSVN"
   },
   "source": [
    "## Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6hCoLNW9USVa"
   },
   "outputs": [],
   "source": [
    "def generator (h5path, indexes,batchSize, shuffle):\n",
    "  db = h5py.File(h5path, \"r\")\n",
    "  while True:\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indexes)\n",
    "    for i in range(0, len(indexes), batchSize):\n",
    "        batch_indexes = indexes[i:i+batchSize]\n",
    "        batch_indexes.sort()\n",
    "        \n",
    "        bx = db[\"images\"][batch_indexes,:,:,:]\n",
    "        by = db[\"labels\"][batch_indexes,:]\n",
    "\n",
    "        yield (bx, by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKEK7CfXUSVq"
   },
   "source": [
    "## Training model (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1952
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 614765,
     "status": "error",
     "timestamp": 1526891363734,
     "user": {
      "displayName": "Raúl Ortega",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "118338186511913721141"
     },
     "user_tz": -120
    },
    "id": "FbUsHU1pUSVq",
    "outputId": "2238c2a0-f3e2-485b-959e-6401eafcb811",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "\n",
    "fold = 1\n",
    "print (\"Number of images: \"+str(n_images))\n",
    "print (\"Number of classes: 5\\n\")\n",
    "for train, test in kfold.split([None] * n_images):\n",
    "    print(\"FOLD: \" +str(fold))\n",
    "    print(\"Training with \"+ str(len(train))+ \" samples and evaluating with \"+str(len(test)))\n",
    "    \n",
    "    model = get_cat_figures_model (weights)\n",
    "    \n",
    "    adam = optimizers.Adam(lr=1e-4, decay=1e-5)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['categorical_accuracy'])\n",
    "  \n",
    "    model.fit_generator(generator(h5path,train,batchSize, shuffle=True), epochs=epochs, steps_per_epoch = len(train)//batchSize, validation_data=(generator(h5path,test,batchSize, shuffle=False)), validation_steps= len(test)//batchSize)\n",
    "      \n",
    "    db = h5py.File(h5path, \"r\")\n",
    "    labels_test = db[\"labels\"][test,:]\n",
    "    db.close()\n",
    "\n",
    "    pred = model.predict_generator(generator(h5path, test, batchSize=batchSize, shuffle=False),steps = len(test)//batchSize)\n",
    "    maximos = np.argmax(pred,axis=1)\n",
    "    predNew = np.zeros(np.shape(pred))\n",
    "    for i in range(len(predNew)):\n",
    "        predNew[i,maximos[i]]=1\n",
    "    print(classification_report(labels_test[0:batchSize*(len(test)//batchSize)], predNew, digits=4))\n",
    "    precisions.append(precision_score(labels_test[0:batchSize*(len(test)//batchSize)], predNew, average=\"weighted\"))\n",
    "    recalls.append(recall_score(labels_test[0:batchSize*(len(test)//batchSize)], predNew, average=\"weighted\"))\n",
    "    f1s.append(f1_score(labels_test[0:batchSize*(len(test)//batchSize)], predNew, average=\"weighted\"))\n",
    "    fold = fold +1\n",
    "print(\"Precision: %.4f (+/- %.2f)\" % (np.mean(precisions), np.std(precisions)))\n",
    "print(\"Recall: %.4f (+/- %.2f)\" % (np.mean(recalls), np.std(recalls)))\n",
    "print(\"F1 Score: %.4f (+/- %.2f)\" % (np.mean(f1s), np.std(f1s)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZfQTLqW9HAJ8",
    "YSxJx5b4bRyC",
    "4AQchbAKHVSk",
    "2fJlkTLGSXKX",
    "KftAruKttTBk",
    "MYRiYDmf9l_l",
    "D8jhia_wphXd",
    "KVdtrbJCpmJW",
    "vqxj9bWWpmJY",
    "FXI8meXQeqKA",
    "o_gDBeWxvEX2"
   ],
   "default_view": {},
   "name": "TFG.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
